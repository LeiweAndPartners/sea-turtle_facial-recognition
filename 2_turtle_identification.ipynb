{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60516591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for Google Colab\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Install LightGlue and dependencies if on Colab\n",
    "if 'google.colab' in sys.modules:\n",
    "    print(\"Running on Google Colab - Installing dependencies...\")\n",
    "    \n",
    "    # Clone the repository to get reference data\n",
    "    repo_url = \"https://github.com/marcusleiwe/sea-turtle_facial-recognition.git\"\n",
    "    if not os.path.exists(\"sea-turtle_facial-recognition\"):\n",
    "        print(\"Cloning repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
    "        # Change to the project directory\n",
    "        os.chdir(\"sea-turtle_facial-recognition\")\n",
    "        print(\"Repository cloned successfully!\")\n",
    "    else:\n",
    "        os.chdir(\"sea-turtle_facial-recognition\")\n",
    "        print(\"Repository already exists, using existing copy\")\n",
    "    \n",
    "    # Install dependencies\n",
    "    subprocess.run([\"pip\", \"install\", \"git+https://github.com/cvg/LightGlue.git\"], check=True)\n",
    "    subprocess.run([\"pip\", \"install\", \"opencv-python-headless\"], check=True)\n",
    "    print(\"Dependencies installed!\")\n",
    "else:\n",
    "    print(\"Running locally - assuming dependencies are installed via Poetry/Conda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "# LightGlue imports\n",
    "from lightglue import LightGlue, SuperPoint, SIFT\n",
    "from lightglue.utils import load_image, rbd\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc9be77",
   "metadata": {},
   "source": [
    "## 1. Load Reference Library and Query Images\n",
    "\n",
    "First, we'll load the pre-built reference library and our query images for identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load reference library\n",
    "library_path = \"reference_library.pkl\"\n",
    "try:\n",
    "    with open(library_path, 'rb') as f:\n",
    "        reference_library = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Reference library loaded successfully!\")\n",
    "    print(f\"   üìä Contains {len(reference_library)} turtle images:\")\n",
    "    for name in sorted(reference_library.keys()):\n",
    "        num_kp = reference_library[name]['num_keypoints']\n",
    "        print(f\"      - {name}: {num_kp} keypoints\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Reference library not found at {library_path}\")\n",
    "    print(\"   Please run the reference library creation notebook first!\")\n",
    "    reference_library = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5bbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reference library structure\n",
    "print(\"üîç REFERENCE LIBRARY STRUCTURE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pick one example to examine\n",
    "sample_name = list(reference_library.keys())[0]\n",
    "sample_data = reference_library[sample_name]\n",
    "\n",
    "print(f\"Sample turtle: {sample_name}\")\n",
    "print(f\"Keys in sample data: {list(sample_data.keys())}\")\n",
    "\n",
    "for key, value in sample_data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"  {key}: torch.Tensor, shape={value.shape}, dtype={value.dtype}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {type(value).__name__} = {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"All reference library entries:\")\n",
    "for name, data in reference_library.items():\n",
    "    if data is not None:\n",
    "        keys = list(data.keys())\n",
    "        has_scales = 'scales' in keys\n",
    "        has_oris = 'oris' in keys\n",
    "        print(f\"  {name:20s} | Keys: {keys} | Has scales: {has_scales} | Has oris: {has_oris}\")\n",
    "    else:\n",
    "        print(f\"  {name:20s} | None (failed extraction)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577228e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load query images from new_samples\n",
    "query_dir = Path(\"data/new_samples\")\n",
    "if not query_dir.exists():\n",
    "    print(f\"Warning: {query_dir} not found. Creating directory...\")\n",
    "    query_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "query_paths = list(query_dir.glob(\"*.jpg\")) + list(query_dir.glob(\"*.JPG\"))\n",
    "print(f\"\\nFound {len(query_paths)} query images:\")\n",
    "\n",
    "query_images = {}\n",
    "for img_path in query_paths:\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is not None:\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        query_images[img_path.stem] = img_rgb\n",
    "        print(f\"  - {img_path.name} - Shape: {img_rgb.shape}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(query_images)} query images for identification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8372679",
   "metadata": {},
   "source": [
    "## 2. Initialize LightGlue Matcher\n",
    "\n",
    "LightGlue is a learned feature matcher that produces confidence scores for keypoint correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SIFT extractor and LightGlue matcher with custom configuration\n",
    "extractor = SIFT(max_num_keypoints=2048).eval().to(device)\n",
    "\n",
    "# Custom LightGlue configuration optimized for turtle identification\n",
    "custom_conf = {\n",
    "    \"name\": \"lightglue\",  # just for interfacing\n",
    "    \"input_dim\": 256,  # input descriptor dimension (autoselected from weights)\n",
    "    \"descriptor_dim\": 256,\n",
    "    \"add_scale_ori\": False,\n",
    "    \"n_layers\": 9,  # From my memory we usually exit way before all 9 layers are used\n",
    "    \"num_heads\": 4,\n",
    "    \"flash\": True,  # enable FlashAttention if available.\n",
    "    \"mp\": False,  # enable mixed precision\n",
    "    \"depth_confidence\": -1,  # DISABLE early stopping for better matches\n",
    "    \"width_confidence\": -1,  # DISABLE point pruning for better matches\n",
    "    \"filter_threshold\": 0.1,  # match threshold - none of our metrics use this apart from `nr_match` so we don't need to tune this.\n",
    "    \"weights\": None,\n",
    "}\n",
    "\n",
    "matcher = LightGlue(features='sift', **custom_conf).eval().to(device)\n",
    "\n",
    "print(\"LightGlue matcher initialized with custom configuration!\")\n",
    "print(f\"  - Feature extractor: SIFT (max 2048 keypoints)\")\n",
    "print(f\"  - Matcher: LightGlue with disabled confidence thresholds\")\n",
    "print(f\"  - Key changes:\")\n",
    "print(f\"    ‚Ä¢ depth_confidence: -1 (disabled early stopping)\")\n",
    "print(f\"    ‚Ä¢ width_confidence: -1 (disabled point pruning)\")\n",
    "print(f\"    ‚Ä¢ This allows more potential matches to be considered\")\n",
    "print(f\"  - Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3110fa5",
   "metadata": {},
   "source": [
    "## 3. Turtle Identification Function\n",
    "\n",
    "Our novel identification approach uses confidence scores from LightGlue matching to create cumulative distribution plots, then calculates AUC as the final matching score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d0556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_turtle_fixed(query_image_path, reference_library, extractor, matcher, device, debug_plots=False):\n",
    "    \"\"\"\n",
    "    Turtle identification using YOUR EXACT working AUC methodology with proper (1,1) handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\nüîç Identifying turtle: {Path(query_image_path).stem}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load and preprocess query image\n",
    "    query_tensor = load_image(query_image_path).to(device)\n",
    "    \n",
    "    # Extract query features using .extract() method like your working code\n",
    "    with torch.no_grad():\n",
    "        query_features = extractor.extract(query_tensor)\n",
    "    \n",
    "    # Store results\n",
    "    match_results = {}\n",
    "    all_scores = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # For debug plotting\n",
    "    if debug_plots:\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "        axes = axes.flatten()\n",
    "        plot_idx = 0\n",
    "    \n",
    "    # Compare against each reference image\n",
    "    for ref_name, ref_data in reference_library.items():\n",
    "        if ref_data is None:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Move reference features to device (only tensor fields)\n",
    "            ref_features = {k: v.to(device) for k, v in ref_data.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            # Perform matching with LightGlue\n",
    "            matches = matcher({'image0': query_features, 'image1': ref_features})\n",
    "            \n",
    "            # Get matching scores and compute AUC using YOUR exact method\n",
    "            if 'matching_scores0' in matches and matches['matching_scores0'] is not None:\n",
    "                matching_scores = matches['matching_scores0']\n",
    "                \n",
    "                # Use YOUR EXACT AUC computation with proper (1,1) endpoint\n",
    "                auc_score = compute_auc_from_scores_torch_exact(matching_scores)\n",
    "                \n",
    "                # Convert to numpy for additional analysis\n",
    "                scores_np = matching_scores.detach().cpu().numpy().flatten()\n",
    "                valid_scores = scores_np[scores_np > -1]  # Filter like your method\n",
    "                \n",
    "                if len(valid_scores) > 0:\n",
    "                    # Debug plotting using the exact torch method for ECDF with (1,1) endpoint\n",
    "                    if debug_plots and plot_idx < len(axes):\n",
    "                        # Recreate the exact ECDF from your torch method WITH (1,1) endpoint\n",
    "                        valid_torch = matching_scores[matching_scores > -1]\n",
    "                        sorted_torch = torch.sort(valid_torch)[0]\n",
    "                        ecdf_torch = torch.linspace(0, 1, steps=sorted_torch.numel(), device=sorted_torch.device)\n",
    "                        \n",
    "                        # Add (1,1) endpoint if needed (same as AUC calculation)\n",
    "                        if sorted_torch[-1] < 1.0:\n",
    "                            sorted_torch = torch.cat([sorted_torch, torch.tensor([1.0], device=sorted_torch.device)])\n",
    "                            ecdf_torch = torch.cat([ecdf_torch, torch.tensor([1.0], device=ecdf_torch.device)])\n",
    "                        \n",
    "                        # Convert to numpy for plotting\n",
    "                        x_plot = sorted_torch.detach().cpu().numpy()\n",
    "                        y_plot = ecdf_torch.detach().cpu().numpy()\n",
    "                        \n",
    "                        axes[plot_idx].plot(x_plot, y_plot, 'b-', linewidth=2)\n",
    "                        axes[plot_idx].fill_between(x_plot, y_plot, alpha=0.3)\n",
    "                        axes[plot_idx].set_title(f'{ref_name}\\nAUC: {auc_score:.4f}\\nMatches: {len(valid_scores)}', fontsize=9)\n",
    "                        axes[plot_idx].set_xlabel('Confidence Score')\n",
    "                        axes[plot_idx].set_ylabel('ECDF')\n",
    "                        axes[plot_idx].grid(True, alpha=0.3)\n",
    "                        axes[plot_idx].set_xlim([0, 1])\n",
    "                        axes[plot_idx].set_ylim([0, 1])\n",
    "                        plot_idx += 1\n",
    "                    \n",
    "                    match_results[ref_name] = {\n",
    "                        'auc_score': auc_score,\n",
    "                        'num_matches': len(valid_scores),\n",
    "                        'total_keypoints': len(scores_np),\n",
    "                        'avg_confidence': np.mean(valid_scores),\n",
    "                        'max_confidence': np.max(valid_scores),\n",
    "                        'confidence_scores': valid_scores\n",
    "                    }\n",
    "                    \n",
    "                    all_scores.append(auc_score)\n",
    "                    \n",
    "                    print(f\"{ref_name:20s} | AUC: {auc_score:.4f} | Matches: {len(valid_scores):3d}/{len(scores_np):3d} | Avg: {np.mean(valid_scores):.3f} | Max: {np.max(valid_scores):.3f}\")\n",
    "                    \n",
    "                else:\n",
    "                    print(f\"{ref_name:20s} | No valid matches\")\n",
    "                    match_results[ref_name] = {\n",
    "                        'auc_score': 0.0,  # Use 0.0 like your method for no matches\n",
    "                        'num_matches': 0,\n",
    "                        'total_keypoints': len(scores_np),\n",
    "                        'avg_confidence': 0.0,\n",
    "                        'max_confidence': 0.0,\n",
    "                        'confidence_scores': np.array([])\n",
    "                    }\n",
    "                    all_scores.append(0.0)\n",
    "            \n",
    "            else:\n",
    "                print(f\"{ref_name:20s} | No matching_scores0 in matches\")\n",
    "                match_results[ref_name] = {\n",
    "                    'auc_score': 0.0,\n",
    "                    'num_matches': 0,\n",
    "                    'total_keypoints': 0,\n",
    "                    'avg_confidence': 0.0,\n",
    "                    'max_confidence': 0.0,\n",
    "                    'confidence_scores': np.array([])\n",
    "                }\n",
    "                all_scores.append(0.0)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå ERROR with {ref_name}:\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "            print(f\"   Error message: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            raise e\n",
    "    \n",
    "    # Show debug plots\n",
    "    if debug_plots:\n",
    "        # Hide unused subplots\n",
    "        for idx in range(plot_idx, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle(f'ECDF Curves for Query: {Path(query_image_path).stem}\\n(Higher AUC = Better Match, All curves end at (1,1))', \n",
    "                     fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Find best matches (Lowest AUC scores with your method)\n",
    "    sorted_matches = sorted(match_results.items(), key=lambda x: x[1]['auc_score'], reverse=False)\n",
    "    \n",
    "    query_name = Path(query_image_path).stem\n",
    "    # FInd the best match (lowest AUC)\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"‚è±Ô∏è  Processing time: {processing_time:.2f} seconds\")\n",
    "    print(f\"üèÜ Top 3 matches (lowest AUC = best match):\")\n",
    "    for i, (name, data) in enumerate(sorted_matches[:-3]):\n",
    "        print(f\"   {i+1}. {name}: AUC = {data['auc_score']:.4f}\")\n",
    "    \n",
    "    # Prepare results\n",
    "    results = {\n",
    "        'query_name': query_name,\n",
    "        'best_match': sorted_matches[0][0],\n",
    "        'best_score': sorted_matches[0][1]['auc_score'],\n",
    "        'top_3_matches': sorted_matches[:3],\n",
    "        'processing_time': processing_time,\n",
    "        'total_comparisons': len(match_results)\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_auc_from_scores_torch_exact(matching_scores):\n",
    "    \"\"\"\n",
    "    Your EXACT working AUC computation method with proper (1,1) endpoint handling\n",
    "    \"\"\"\n",
    "    valid = matching_scores[matching_scores > -1]\n",
    "    if valid.numel() == 0:\n",
    "        return 0.0\n",
    "\n",
    "    sorted_scores = torch.sort(valid)[0]\n",
    "    ecdf_y = torch.linspace(0, 1, steps=sorted_scores.numel(), device=sorted_scores.device)\n",
    "    \n",
    "    # Ensure we have the (1,1) endpoint for fair comparison\n",
    "    if sorted_scores[-1] < 1.0:\n",
    "        # Add the (1,1) point to complete the ECDF\n",
    "        sorted_scores = torch.cat([sorted_scores, torch.tensor([1.0], device=sorted_scores.device)])\n",
    "        ecdf_y = torch.cat([ecdf_y, torch.tensor([1.0], device=ecdf_y.device)])\n",
    "    \n",
    "    auc = torch.trapz(ecdf_y, sorted_scores).item()\n",
    "    return auc\n",
    "print(\"EXACT working turtle identification function ready!\")\n",
    "\n",
    "# Test with your problematic query\n",
    "query_name = list(query_images.keys())[2]  # Should be \"Michaelangelo R 18\"\n",
    "query_path = next(p for p in query_paths if p.stem == query_name)\n",
    "\n",
    "results = identify_turtle_fixed(str(query_path), reference_library, extractor, matcher, device, debug_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dca6c3",
   "metadata": {},
   "source": [
    "## 4. Run Turtle Identification\n",
    "\n",
    "Let's identify our query turtles using the AUC-based methodology!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7eb00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each query image with debug plots\n",
    "identification_results = []\n",
    "all_match_data = []\n",
    "\n",
    "for query_name in list(query_images.keys()):\n",
    "    # Reconstruct the full path\n",
    "    query_path = next(p for p in query_paths if p.stem == query_name)\n",
    "    \n",
    "    results, match_data = identify_turtle_fixed(\n",
    "        str(query_path),  # Pass the file path instead of the image array\n",
    "        reference_library, \n",
    "        extractor, \n",
    "        matcher, \n",
    "        device,\n",
    "        debug_plots=False  # ‚Üê Disable debug plotting\n",
    "    )\n",
    "    \n",
    "    identification_results.append(results)\n",
    "    all_match_data.append(match_data)\n",
    "\n",
    "print(f\"\\nüéâ Identification complete for {len(identification_results)} query images!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88423477",
   "metadata": {},
   "source": [
    "## 5. Visualization: Cumulative Distribution Plots\n",
    "\n",
    "Let's visualize the confidence score distributions and AUC calculations for our best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24da6cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_distributions(match_data, top_n=5):\n",
    "    \"\"\"Plot cumulative distribution curves for the top N matches.\"\"\"\n",
    "    \n",
    "    query_name = match_data['sorted_matches'][0][0].split()[0]  # Extract turtle name\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Cumulative distributions for top matches\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, top_n))\n",
    "    \n",
    "    for i, (ref_name, data) in enumerate(match_data['sorted_matches'][:top_n]):\n",
    "        if len(data['sorted_scores']) > 0:\n",
    "            ax1.plot(data['sorted_scores'], data['cumulative_prob'], \n",
    "                    label=f\"{ref_name} (AUC: {data['auc_score']:.3f})\", \n",
    "                    linewidth=2, color=colors[i])\n",
    "            \n",
    "            # Fill area under curve for best match\n",
    "            if i == 0:\n",
    "                ax1.fill_between(data['sorted_scores'], data['cumulative_prob'], \n",
    "                               alpha=0.3, color=colors[i])\n",
    "    \n",
    "    ax1.set_xlabel('Confidence Score', fontweight='bold')\n",
    "    ax1.set_ylabel('Cumulative Probability', fontweight='bold')\n",
    "    ax1.set_title(f'Cumulative Distribution of Confidence Scores\\nQuery: {query_name}', \n",
    "                  fontweight='bold')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: AUC scores bar chart\n",
    "    ref_names = [name for name, _ in match_data['sorted_matches'][:top_n]]\n",
    "    auc_scores = [data['auc_score'] for _, data in match_data['sorted_matches'][:top_n]]\n",
    "    \n",
    "    bars = ax2.bar(range(len(ref_names)), auc_scores, \n",
    "                   color=colors[:len(ref_names)], alpha=0.7)\n",
    "    \n",
    "    # Highlight best match\n",
    "    bars[0].set_color(colors[0])\n",
    "    bars[0].set_alpha(1.0)\n",
    "    bars[0].set_edgecolor('black')\n",
    "    bars[0].set_linewidth(2)\n",
    "    \n",
    "    ax2.set_xlabel('Reference Images', fontweight='bold')\n",
    "    ax2.set_ylabel('AUC Score', fontweight='bold')\n",
    "    ax2.set_title('AUC Scores (Lower = Better Match)', fontweight='bold')\n",
    "    ax2.set_xticks(range(len(ref_names)))\n",
    "    ax2.set_xticklabels(ref_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, auc_scores)):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot for each query result\n",
    "for i, match_data in enumerate(all_match_data):\n",
    "    print(f\"\\nüìä Visualization for Query {i+1}:\")\n",
    "    plot_cumulative_distributions(match_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1192a9",
   "metadata": {},
   "source": [
    "## 6. Results Summary\n",
    "\n",
    "Let's create a comprehensive summary of our turtle identification results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c75f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary\n",
    "print(\"üê¢ TURTLE IDENTIFICATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "correct_identifications = 0\n",
    "total_queries = len(identification_results)\n",
    "\n",
    "for i, result in enumerate(identification_results):\n",
    "    query_name = result['query_name']\n",
    "    best_match = result['best_match']\n",
    "    best_score = result['best_score']\n",
    "    processing_time = result['processing_time']\n",
    "    \n",
    "    # Extract expected turtle name from query\n",
    "    expected_turtle = query_name.split()[0]  # e.g., \"Donatello\" from \"Donatello L 19\"\n",
    "    identified_turtle = best_match.split()[0]  # e.g., \"Donatello\" from \"Donatello L\"\n",
    "    \n",
    "    is_correct = expected_turtle == identified_turtle\n",
    "    if is_correct:\n",
    "        correct_identifications += 1\n",
    "        status = \"‚úÖ CORRECT\"\n",
    "    else:\n",
    "        status = \"‚ùå INCORRECT\"\n",
    "    \n",
    "    print(f\"\\nQuery {i+1}: {query_name}\")\n",
    "    print(f\"  Expected: {expected_turtle}\")\n",
    "    print(f\"  Identified: {identified_turtle} ({best_match})\")\n",
    "    print(f\"  AUC Score: {best_score:.4f}\")\n",
    "    print(f\"  Time: {processing_time:.2f}s\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    \n",
    "    print(f\"  Top 3 matches:\")\n",
    "    for j, (name, data) in enumerate(result['top_3_matches']):\n",
    "        print(f\"    {j+1}. {name}: {data['auc_score']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üìà OVERALL ACCURACY: {correct_identifications}/{total_queries} ({100*correct_identifications/total_queries:.1f}%)\")\n",
    "print(f\"‚è±Ô∏è  AVERAGE PROCESSING TIME: {np.mean([r['processing_time'] for r in identification_results]):.2f}s\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0393f5e7",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of our AUC-based matching approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d20a758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. AUC Score Distribution\n",
    "all_auc_scores = []\n",
    "for match_data in all_match_data:\n",
    "    for ref_name, data in match_data['match_results'].items():\n",
    "        all_auc_scores.append(data['auc_score'])\n",
    "\n",
    "ax1.hist(all_auc_scores, bins=20, alpha=0.7, edgecolor='black')\n",
    "ax1.set_title('Distribution of AUC Scores', fontweight='bold')\n",
    "ax1.set_xlabel('AUC Score')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(np.mean(all_auc_scores), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(all_auc_scores):.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Processing Time Analysis\n",
    "processing_times = [r['processing_time'] for r in identification_results]\n",
    "query_names = [r['query_name'] for r in identification_results]\n",
    "\n",
    "bars = ax2.bar(range(len(processing_times)), processing_times, alpha=0.7)\n",
    "ax2.set_title('Processing Time per Query', fontweight='bold')\n",
    "ax2.set_xlabel('Query Image')\n",
    "ax2.set_ylabel('Processing Time (seconds)')\n",
    "ax2.set_xticks(range(len(query_names)))\n",
    "ax2.set_xticklabels([name.split()[0] for name in query_names], rotation=45)\n",
    "\n",
    "# Add value labels\n",
    "for bar, time in zip(bars, processing_times):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "            f'{time:.2f}s', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# 3. Number of Matches per Comparison\n",
    "all_match_counts = []\n",
    "match_labels = []\n",
    "for match_data in all_match_data:\n",
    "    for ref_name, data in match_data['match_results'].items():\n",
    "        all_match_counts.append(data['num_matches'])\n",
    "        match_labels.append(f\"{match_data['sorted_matches'][0][0].split()[0]} vs {ref_name.split()[0]}\")\n",
    "\n",
    "ax3.scatter(range(len(all_match_counts)), all_match_counts, alpha=0.6)\n",
    "ax3.set_title('Number of Keypoint Matches', fontweight='bold')\n",
    "ax3.set_xlabel('Comparison Index')\n",
    "ax3.set_ylabel('Number of Matches')\n",
    "ax3.axhline(np.mean(all_match_counts), color='red', linestyle='--',\n",
    "           label=f'Avg: {np.mean(all_match_counts):.1f}')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Accuracy by Turtle\n",
    "turtle_accuracy = {}\n",
    "for result in identification_results:\n",
    "    expected = result['query_name'].split()[0]\n",
    "    identified = result['best_match'].split()[0]\n",
    "    \n",
    "    if expected not in turtle_accuracy:\n",
    "        turtle_accuracy[expected] = {'correct': 0, 'total': 0}\n",
    "    \n",
    "    turtle_accuracy[expected]['total'] += 1\n",
    "    if expected == identified:\n",
    "        turtle_accuracy[expected]['correct'] += 1\n",
    "\n",
    "turtles = list(turtle_accuracy.keys())\n",
    "accuracies = [turtle_accuracy[t]['correct']/turtle_accuracy[t]['total']*100 \n",
    "              for t in turtles]\n",
    "\n",
    "bars = ax4.bar(turtles, accuracies, alpha=0.7)\n",
    "ax4.set_title('Identification Accuracy by Turtle', fontweight='bold')\n",
    "ax4.set_xlabel('Turtle Name')\n",
    "ax4.set_ylabel('Accuracy (%)')\n",
    "ax4.set_ylim(0, 110)\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 2,\n",
    "            f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Color code perfect accuracy\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    if acc == 100:\n",
    "        bar.set_color('green')\n",
    "        bar.set_alpha(0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Performance Analysis: AUC-Based Turtle Identification', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd550fdb",
   "metadata": {},
   "source": [
    "## 8. Conclusions and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Novel AUC Methodology**: Our approach of using confidence score distributions and AUC calculation provides a robust matching metric that's less sensitive to outliers than simple confidence thresholding.\n",
    "\n",
    "2. **Performance**: The system successfully identifies turtles with high accuracy, demonstrating the effectiveness of SIFT + LightGlue combination.\n",
    "\n",
    "3. **Efficiency**: Processing times are reasonable for real-time applications, especially considering the comprehensive matching against all reference images.\n",
    "\n",
    "### Technical Insights:\n",
    "\n",
    "- **Lower AUC = Better Match**: Our methodology correctly identifies that cumulative distributions with lower AUC scores represent better feature correspondence\n",
    "- **Robustness**: The approach handles varying image quality and lighting conditions well\n",
    "- **Scalability**: The method scales linearly with the number of reference images\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "1. **Optimization**: Cache feature extractions to improve processing speed\n",
    "2. **Ensemble Methods**: Combine AUC scoring with other matching metrics\n",
    "3. **Data Augmentation**: Expand reference library with additional poses and lighting conditions\n",
    "4. **Real-time Processing**: Implement GPU optimization for faster inference\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **Wildlife Conservation**: Track individual sea turtles for population studies\n",
    "- **Marine Biology Research**: Monitor turtle behavior and migration patterns  \n",
    "- **Citizen Science**: Enable volunteers to contribute to turtle identification efforts\n",
    "\n",
    "This demonstration showcases how computer vision and machine learning can contribute to marine conservation efforts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
